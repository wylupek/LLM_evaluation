{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLUE benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports and utils"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import nltk\n",
    "\n",
    "def predict_and_print(logits, labels=None):\n",
    "    scores = torch.nn.functional.sigmoid(logits)\n",
    "    predictions = torch.argmax(scores, dim=1).tolist()\n",
    "    print(f\"Predictions:   {predictions}\")\n",
    "    if labels is not None:\n",
    "        print(f\"Actual labels: {labels}\")\n",
    "    print(f\"\\nScores:\\n{scores}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-11T09:19:42.143447200Z",
     "start_time": "2024-07-11T09:19:42.140932800Z"
    }
   },
   "outputs": [],
   "execution_count": 70
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoLA - Corpus of Linguistic Acceptability\n",
    "It used to evaluate models on the task of linguistic acceptability. Each sentence is labeled as either:<br>\n",
    "0 - incorrect - unacceptable<br>\n",
    "1 - correct - acceptable\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "ExecuteTime": {
     "end_time": "2024-07-11T09:30:18.446982200Z",
     "start_time": "2024-07-11T09:30:17.594574600Z"
    }
   },
   "source": [
    "# Use a BERT fine-tuned on CoLA\n",
    "model_checkpoint = \"textattack/bert-base-uncased-CoLA\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "cola_sentences = [\n",
    "    \"The boy is playing.\",\n",
    "    \"The dog run fast.\"\n",
    "]\n",
    "cola_labels = [1, 0]\n",
    "\n",
    "encodings = tokenizer(cola_sentences, truncation=True, padding=True, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encodings)\n",
    "\n",
    "predict_and_print(outputs.logits, cola_labels)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:   [1, 0]\n",
      "Actual labels: [1, 0]\n",
      "\n",
      "Scores:\n",
      "tensor([[0.1056, 0.8955],\n",
      "        [0.8404, 0.1765]])\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QNLI - Question Natural Language Inference\n",
    "This dataset is designed for question-answering tasks. Each pair consists of a question and a sentence, labeled as either: <br>\n",
    "0 - entailment - acceptable<br>\n",
    "1 - not_entailment - unacceptable "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "ExecuteTime": {
     "end_time": "2024-07-11T09:30:25.063265Z",
     "start_time": "2024-07-11T09:30:24.332066Z"
    }
   },
   "source": [
    "# Use a BERT fine-tuned on QNLI\n",
    "model_checkpoint = \"textattack/bert-base-uncased-QNLI\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "qnli_questions = [\n",
    "    'How many people live in Berlin?', \n",
    "    'What is the size of New York?'\n",
    "]\n",
    "qnli_sentences = [\n",
    "    'Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', \n",
    "    'New York City is famous for the Metropolitan Museum of Art.'\n",
    "]\n",
    "qnli_labels = [0, 1]\n",
    "\n",
    "encodings = tokenizer(qnli_questions, qnli_sentences, truncation=True, padding=True, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encodings)\n",
    "        \n",
    "predict_and_print(outputs.logits, qnli_labels)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:   [0, 1]\n",
      "Actual labels: [0, 1]\n",
      "\n",
      "Scores:\n",
      "tensor([[0.9419, 0.0948],\n",
      "        [0.0635, 0.9644]])\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNLI - Multi-Genre Natural Language Inference\n",
    "This dataset is used for evaluating models on the task of natural language inference. Each pair of sentences is labeled as:<br>\n",
    "0 - contradiction - unacceptable<br>\n",
    "1 - neutral - acceptable<br>\n",
    "2 - entailment - acceptable"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "ExecuteTime": {
     "end_time": "2024-07-11T09:30:27.959638Z",
     "start_time": "2024-07-11T09:30:26.928784200Z"
    }
   },
   "source": [
    "# Use a BERT fine-tuned on MNLI\n",
    "model_checkpoint = \"textattack/bert-base-uncased-MNLI\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "mnli_premises = [\n",
    "    \"Sleep is a vital component of our overall health and well-being\",\n",
    "    \"Sleep is a vital component of our overall health and well-being\",\n",
    "    \"Sleep is a vital component of our overall health and well-being\"\n",
    "]\n",
    "mnli_hypotheses = [\n",
    "    \"The boy is sleeping.\",\n",
    "    \"Sleep is not a luxury but a necessity.\",\n",
    "    \"Sleep isn't important.\"\n",
    "]\n",
    "mnli_labels = [2, 1, 0]\n",
    "\n",
    "encodings = tokenizer(mnli_premises, mnli_hypotheses, truncation=True, padding=True, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encodings)\n",
    "        \n",
    "predict_and_print(outputs.logits, mnli_labels)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:   [2, 1, 0]\n",
      "Actual labels: [2, 1, 0]\n",
      "\n",
      "Scores:\n",
      "tensor([[0.4699, 0.0906, 0.8886],\n",
      "        [0.3002, 0.6163, 0.5773],\n",
      "        [0.9957, 0.0394, 0.1105]])\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "cell_type": "markdown",
   "source": [
    "To evaluate a chatbot's answers using the MNLI dataset, we first need to split the text into individual sentences. Next, we calculate the scores for each pair of sentences. For short answers, each pair should be classified as either 'neutral' or 'entailment'."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sleep is crucial for maintaining physical health, as it allows the body to repair and rejuvenate itself.\n",
      "It plays a vital role in cognitive functions, including memory consolidation and learning.\n",
      "Adequate sleep supports emotional well-being by helping to regulate mood and stress levels.\n",
      "Consistent, quality sleep strengthens the immune system, making it easier to fend off illnesses.\n",
      "Additionally, proper sleep is essential for maintaining energy levels and overall productivity throughout the day.\n",
      "\n",
      "Predictions:   [2, 2, 2, 2]\n",
      "\n",
      "Scores:\n",
      "tensor([[0.2991, 0.1641, 0.8924],\n",
      "        [0.2571, 0.1057, 0.9436],\n",
      "        [0.0662, 0.1924, 0.9620],\n",
      "        [0.0338, 0.2374, 0.9779]])\n"
     ]
    }
   ],
   "source": [
    "text = (\"Sleep is crucial for maintaining physical health, as it allows the body to repair and rejuvenate itself.\"\n",
    "        \" It plays a vital role in cognitive functions, including memory consolidation and learning.\"\n",
    "        \" Adequate sleep supports emotional well-being by helping to regulate mood and stress levels.\"\n",
    "        \" Consistent, quality sleep strengthens the immune system, making it easier to fend off illnesses.\"\n",
    "        \" Additionally, proper sleep is essential for maintaining energy levels and overall productivity throughout the day.\")\n",
    "\n",
    "# nltk.download('punkt')\n",
    "mnli_sentences = nltk.sent_tokenize(text)\n",
    "for sentence in mnli_sentences: print(sentence)\n",
    "\n",
    "encodings = tokenizer(mnli_sentences[:-1], mnli_sentences[1:], truncation=True, padding=True, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encodings)\n",
    "\n",
    "print(\"\")\n",
    "predict_and_print(outputs.logits)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-11T09:30:32.270531200Z",
     "start_time": "2024-07-11T09:30:32.154193600Z"
    }
   },
   "execution_count": 75
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRPC - Microsoft Research Paraphrase Corpus\n",
    "Dataset commonly used for evaluating paraphrase detection models. It consists of pairs of sentences, each labeled as either:<br>\n",
    "0 - not_paraphrases - unacceptable<br>\n",
    "1 - paraphrases - acceptable"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Use a BERT fine-tuned on MRPC\n",
    "model_checkpoint = \"textattack/bert-base-uncased-MRPC\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "mrpc_1 = [\n",
    "    \"The company announced its quarterly earnings today.\",\n",
    "    \"She loves to read books during her free time.\",\n",
    "    \"The new policy will affect all employees starting next month.\"\n",
    "]\n",
    "mrpc_2 = [\n",
    "    \"The company revealed its quarterly results today\",\n",
    "    \"He enjoys playing soccer with his friends after school.\",\n",
    "    \"Starting next month, the new policy will impact all staff members.\"\n",
    "]\n",
    "mrpc_labels = [1, 0, 1]\n",
    "\n",
    "encodings = tokenizer(mrpc_2, mrpc_1, truncation=True, padding=True, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encodings)\n",
    "        \n",
    "predict_and_print(outputs.logits, mrpc_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-11T09:30:35.710521800Z",
     "start_time": "2024-07-11T09:30:34.977622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:   [1, 0, 1]\n",
      "Actual labels: [1, 0, 1]\n",
      "\n",
      "Scores:\n",
      "tensor([[0.0937, 0.9303],\n",
      "        [0.6801, 0.1622],\n",
      "        [0.0947, 0.9326]])\n"
     ]
    }
   ],
   "execution_count": 76
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
