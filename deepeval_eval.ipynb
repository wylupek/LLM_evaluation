{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## DeepEval"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a1f04f39ab17b34"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports and logging in"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "451df0d9de0d8718"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "import torch\n",
    "\n",
    "load_dotenv()\n",
    "login(os.getenv(\"HUGGINGFACE_API_KEY\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8504d05fe5ec6fb1",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading Llama-3 8B"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0bacf35eb136d23"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "checkpoint = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint,\n",
    "    device_map=\"auto\",\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    quantization_config=quantization_config,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e74b4427dd75074",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing loaded model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6cf01c3e6531f28"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "terminators_test = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "messages_test = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "input_ids_test = tokenizer.apply_chat_template(\n",
    "    messages_test,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "\n",
    "outputs_test = model.generate(\n",
    "    input_ids_test,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators_test,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "response_test = outputs_test[0][input_ids_test.shape[-1]:]\n",
    "print(tokenizer.decode(response_test, skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15de71dd18f0029b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "messages_test = [\n",
    "    {\"role\": \"user\", \"content\": \"Generate very short JSON file.\"},\n",
    "]\n",
    "\n",
    "input_ids_test = tokenizer.apply_chat_template(\n",
    "    messages_test,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "outputs_test = model.generate(\n",
    "    input_ids_test,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators_test,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "response_test = outputs_test[0][input_ids_test.shape[-1]:]\n",
    "print(tokenizer.decode(response_test, skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73e20a9b098563bc",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Class for DeepEval"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2df64017f6988727"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class CustomLlama(DeepEvalBaseLLM):\n",
    "    def __init__(self, init_model, init_tokenizer):\n",
    "        self.model = init_model\n",
    "        self.tokenizer = init_tokenizer\n",
    "        self.terminators = [\n",
    "            init_tokenizer.eos_token_id,\n",
    "            init_tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        prompt = [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            prompt,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=256,\n",
    "            eos_token_id=self.terminators,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "        return tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        return self.generate(prompt)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Llama-3 8B\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc5d886471012c09",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "custom_LLM = CustomLlama(init_model=model, init_tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7991354556953028",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(custom_LLM.generate(\"Tell a joke.\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c94a69c86cc6f3ff",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### DeepEval metrics\n",
    "#### Hallucination"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c6e7ac10e43d168"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from deepeval.metrics import HallucinationMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Replace this with the actual documents that you are passing as input to your LLM.\n",
    "context=[\"A man with blond-hair, and a brown shirt drinking out of a public water fountain.\"]\n",
    "\n",
    "# Replace this with the actual output from your LLM application\n",
    "output=\"A blond drinking water in public.\"\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What was the blond doing?\",\n",
    "    actual_output=output,\n",
    "    context=context\n",
    ")\n",
    "metric = HallucinationMetric(threshold=0.5, model=custom_LLM)\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "# or evaluate test cases in bulk\n",
    "# evaluate([test_case], [metric])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f42e30f72eee19b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "''' Output:\n",
    "0.0\n",
    "The score is 0.00 because the actual output aligns with the provided context, indicating no hallucination.\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "687b9606afe95326",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Summarization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "259627faf0c728d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from deepeval.metrics import SummarizationMetric\n",
    "\n",
    "# This is the original text to be summarized\n",
    "input_text = \"\"\"\n",
    "The 'coverage score' is calculated as the percentage of assessment questions\n",
    "for which both the summary and the original document provide a 'yes' answer. This\n",
    "method ensures that the summary not only includes key information from the original\n",
    "text but also accurately represents it. A higher coverage score indicates a\n",
    "more comprehensive and faithful summary, signifying that the summary effectively\n",
    "encapsulates the crucial points and details from the original content.\n",
    "\"\"\"\n",
    "\n",
    "# This is the summary, replace this with the actual output from your LLM application\n",
    "output = \"\"\"\n",
    "The coverage score quantifies how well a summary captures and\n",
    "accurately represents key information from the original text,\n",
    "with a higher score indicating greater comprehensiveness.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "test_case = LLMTestCase(input=input_text, actual_output=output)\n",
    "metric = SummarizationMetric(\n",
    "    threshold=0.5,\n",
    "    model=custom_LLM,\n",
    "    assessment_questions=[\n",
    "        \"Is the coverage score based on a percentage of 'yes' answers?\",\n",
    "        \"Does the score ensure the summary's accuracy with the source?\",\n",
    "        \"Does a higher score mean a more comprehensive summary?\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69bd1d254a3bdd44",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "''' Output:\n",
    "0.3333333333333333\n",
    "The score is 0.33 because the summary fails to accurately capture the original text's content, as it introduces new information and contradicts itself. The lack of attention to detail and inconsistencies in the summary lead to a poor summarization score.\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18f03d977ed33081"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Answer Relevancy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98de52fd5cf96e8d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "\n",
    "# Replace this with the actual output from your LLM application\n",
    "output = \"If shoes don't fit you can refund them at no extra cost.\"\n",
    "input_text = \"What if these shoes don't fit?\"\n",
    "\n",
    "metric = AnswerRelevancyMetric(\n",
    "    threshold=0.5,\n",
    "    model=custom_LLM,\n",
    "    include_reason=True\n",
    ")\n",
    "test_case = LLMTestCase(\n",
    "    input=input_text,\n",
    "    actual_output=output\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3cd3dcced40505b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "''' Output:\n",
    "0.5\n",
    "The score is 0.50 because the model provided an irrelevant statement about refunds, which distracted from the main concern of the input question, which is about what to do if the shoes don't fit.\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "290ed67c10932462"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### G-Eval"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "637bae1461eb42e2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "\n",
    "correctness_metric = GEval(\n",
    "    name=\"Correctness\",\n",
    "    criteria=\"Determine whether the actual output is factually correct based on the expected output.\",\n",
    "    # NOTE: you can only provide either criteria or evaluation_steps, and not both\n",
    "    # evaluation_steps=[\n",
    "    #     \"Check whether the facts in 'actual output' contradicts any facts in 'expected output'\",\n",
    "    #     \"You should also heavily penalize omission of detail\",\n",
    "    #     \"Vague language, or contradicting OPINIONS, are OK\"\n",
    "    # ],\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    model=custom_LLM,\n",
    ")\n",
    "\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=\"The dog chased the cat up the tree, who ran up the tree?\",\n",
    "    actual_output=\"It depends, some might consider the cat, while others might argue the dog.\",\n",
    "    expected_output=\"The cat.\"\n",
    ")\n",
    "\n",
    "correctness_metric.measure(test_case)\n",
    "print(correctness_metric.score)\n",
    "print(correctness_metric.reason)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60f8c15a18d55814",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "''' Output:\n",
    "0.0\n",
    "The actual output does not accurately represent the expected output, as it does not provide a direct answer to the question, instead offering a subjective interpretation.\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8f4371a013afc8e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
