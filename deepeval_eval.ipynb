{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## DeepEval"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90b12019e0a833dc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports and logging in"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54e819f54e1d3392"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to C:\\Users\\filip\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "import torch\n",
    "\n",
    "load_dotenv()\n",
    "login(os.getenv(\"HUGGINGFACE_API_KEY\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T08:53:06.415120500Z",
     "start_time": "2024-07-25T08:53:03.002243200Z"
    }
   },
   "id": "4ccc4443d656ed46",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading Llama-3 8B"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22558d267a2d3b4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f35b28f09d41460b9978b6b327c98a43"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint,\n",
    "    device_map=\"auto\",\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    quantization_config=quantization_config,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T08:53:51.034348600Z",
     "start_time": "2024-07-25T08:53:07.425206Z"
    }
   },
   "id": "eb9c430840167a97",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing loaded model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8fcae7c813e56bfe"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrrr, me hearty! Me name be Captain Chatty, the scurviest pirate chatbot to ever sail the Seven Seas! Me be here to swab yer decks with me wit and me wisdom, so hoist the sails and set course fer a swashbucklin' good time!\n"
     ]
    }
   ],
   "source": [
    "terminators_test = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "messages_test = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "input_ids_test = tokenizer.apply_chat_template(\n",
    "    messages_test,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "\n",
    "outputs_test = model.generate(\n",
    "    input_ids_test,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators_test,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "response_test = outputs_test[0][input_ids_test.shape[-1]:]\n",
    "print(tokenizer.decode(response_test, skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T08:54:09.863316500Z",
     "start_time": "2024-07-25T08:54:07.150209300Z"
    }
   },
   "id": "ae7c30f869273472",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a very short JSON file:\n",
      "```\n",
      "{\n",
      "  \"name\": \"John\",\n",
      "  \"age\": 30\n",
      "}\n",
      "```\n",
      "This JSON file has only two key-value pairs: \"name\" with value \"John\", and \"age\" with value 30.\n"
     ]
    }
   ],
   "source": [
    "messages_test = [\n",
    "    {\"role\": \"user\", \"content\": \"Generate very short JSON file.\"},\n",
    "]\n",
    "\n",
    "input_ids_test = tokenizer.apply_chat_template(\n",
    "    messages_test,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "outputs_test = model.generate(\n",
    "    input_ids_test,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators_test,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "response_test = outputs_test[0][input_ids_test.shape[-1]:]\n",
    "print(tokenizer.decode(response_test, skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T08:54:20.542862800Z",
     "start_time": "2024-07-25T08:54:18.485472Z"
    }
   },
   "id": "715dde0d5757e2c5",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Class for DeepEval"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19555a1527d13e2b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class CustomLlama(DeepEvalBaseLLM):\n",
    "    def __init__(self, init_model, init_tokenizer):\n",
    "        self.model = init_model\n",
    "        self.tokenizer = init_tokenizer\n",
    "        self.terminators = [\n",
    "            init_tokenizer.eos_token_id,\n",
    "            init_tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        prompt = [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            prompt,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=256,\n",
    "            eos_token_id=self.terminators,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "        return tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        return self.generate(prompt)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Llama-3 8B\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T08:57:12.837732200Z",
     "start_time": "2024-07-25T08:57:12.830219200Z"
    }
   },
   "id": "752d110d7b93df38",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "custom_LLM = CustomLlama(init_model=model, init_tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T08:57:14.922631700Z",
     "start_time": "2024-07-25T08:57:14.921120Z"
    }
   },
   "id": "5c54474a919a3273",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n"
     ]
    }
   ],
   "source": [
    "print(custom_LLM.generate(\"Tell a joke.\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T08:58:17.316114900Z",
     "start_time": "2024-07-25T08:58:16.690389200Z"
    }
   },
   "id": "fd27ce1cdf29a15e",
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "### DeepEval metrics\n",
    "#### Hallucination"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c60e1d8b5fa31589"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from deepeval.metrics import HallucinationMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Replace this with the actual documents that you are passing as input to your LLM.\n",
    "context=[\"A man with blond-hair, and a brown shirt drinking out of a public water fountain.\"]\n",
    "\n",
    "# Replace this with the actual output from your LLM application\n",
    "output=\"A blond drinking water in public.\"\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What was the blond doing?\",\n",
    "    actual_output=output,\n",
    "    context=context\n",
    ")\n",
    "metric = HallucinationMetric(threshold=0.5, model=custom_LLM)\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "# or evaluate test cases in bulk\n",
    "# evaluate([test_case], [metric])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99f3524d8e3c2102",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "''' Output:\n",
    "0.0\n",
    "The score is 0.00 because the actual output aligns with the provided context, indicating no hallucination.\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4871fd19549a03aa",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Summarization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f5dde3a29251333"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from deepeval.metrics import SummarizationMetric\n",
    "\n",
    "# This is the original text to be summarized\n",
    "input_text = \"\"\"\n",
    "The 'coverage score' is calculated as the percentage of assessment questions\n",
    "for which both the summary and the original document provide a 'yes' answer. This\n",
    "method ensures that the summary not only includes key information from the original\n",
    "text but also accurately represents it. A higher coverage score indicates a\n",
    "more comprehensive and faithful summary, signifying that the summary effectively\n",
    "encapsulates the crucial points and details from the original content.\n",
    "\"\"\"\n",
    "\n",
    "# This is the summary, replace this with the actual output from your LLM application\n",
    "output = \"\"\"\n",
    "The coverage score quantifies how well a summary captures and\n",
    "accurately represents key information from the original text,\n",
    "with a higher score indicating greater comprehensiveness.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "test_case = LLMTestCase(input=input_text, actual_output=output)\n",
    "metric = SummarizationMetric(\n",
    "    threshold=0.5,\n",
    "    model=custom_LLM,\n",
    "    assessment_questions=[\n",
    "        \"Is the coverage score based on a percentage of 'yes' answers?\",\n",
    "        \"Does the score ensure the summary's accuracy with the source?\",\n",
    "        \"Does a higher score mean a more comprehensive summary?\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d97e20a10522754f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "''' Output:\n",
    "0.3333333333333333\n",
    "The score is 0.33 because the summary fails to accurately capture the original text's content, as it introduces new information and contradicts itself. The lack of attention to detail and inconsistencies in the summary lead to a poor summarization score.\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef62aac56a88f7d5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Answer Relevancy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb9c98a96b7f3e00"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "\n",
    "# Replace this with the actual output from your LLM application\n",
    "output = \"If shoes don't fit you can refund them at no extra cost.\"\n",
    "input_text = \"What if these shoes don't fit?\"\n",
    "\n",
    "metric = AnswerRelevancyMetric(\n",
    "    threshold=0.5,\n",
    "    model=custom_LLM,\n",
    "    include_reason=True\n",
    ")\n",
    "test_case = LLMTestCase(\n",
    "    input=input_text,\n",
    "    actual_output=output\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb8c56681cc17aa2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "''' Output:\n",
    "0.5\n",
    "The score is 0.50 because the model provided an irrelevant statement about refunds, which distracted from the main concern of the input question, which is about what to do if the shoes don't fit.\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b37d24c6db606cf4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### G-Eval"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9200fb183035cef9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "\n",
    "correctness_metric = GEval(\n",
    "    name=\"Correctness\",\n",
    "    criteria=\"Determine whether the actual output is factually correct based on the expected output.\",\n",
    "    # NOTE: you can only provide either criteria or evaluation_steps, and not both\n",
    "    # evaluation_steps=[\n",
    "    #     \"Check whether the facts in 'actual output' contradicts any facts in 'expected output'\",\n",
    "    #     \"You should also heavily penalize omission of detail\",\n",
    "    #     \"Vague language, or contradicting OPINIONS, are OK\"\n",
    "    # ],\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    model=custom_LLM,\n",
    ")\n",
    "\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=\"The dog chased the cat up the tree, who ran up the tree?\",\n",
    "    actual_output=\"It depends, some might consider the cat, while others might argue the dog.\",\n",
    "    expected_output=\"The cat.\"\n",
    ")\n",
    "\n",
    "correctness_metric.measure(test_case)\n",
    "print(correctness_metric.score)\n",
    "print(correctness_metric.reason)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d664043877d7176",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "''' Output:\n",
    "0.0\n",
    "The actual output does not accurately represent the expected output, as it does not provide a direct answer to the question, instead offering a subjective interpretation.\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a1edcb3a6f03be2",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
