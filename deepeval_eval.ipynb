{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## DeepEval"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90b12019e0a833dc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports and logging in"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54e819f54e1d3392"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to C:\\Users\\filip\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "load_dotenv()\n",
    "login(os.getenv(\"HUGGINGFACE_API_KEY\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T12:46:39.429444800Z",
     "start_time": "2024-07-24T12:46:39.288374400Z"
    }
   },
   "id": "4ccc4443d656ed46",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading Llama-3 8B"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22558d267a2d3b4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5e66b5d7a4ca413680e6029dc16f6594"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint,\n",
    "    device_map=\"auto\",\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    quantization_config=quantization_config,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T12:46:51.219960400Z",
     "start_time": "2024-07-24T12:46:42.143744600Z"
    }
   },
   "id": "eb9c430840167a97",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing loaded model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8fcae7c813e56bfe"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrrr, shiver me timbers! Me name be Captain Chatbot, the scourge o' the seven seas... o' text! I be a swashbucklin' bot, here to chat with ye about all manner o' things, from the finest booty to the most treacherous waters. So hoist the colors, me hearty, and let's set sail fer a chat!\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T13:11:03.454004400Z",
     "start_time": "2024-07-24T13:11:00.153729200Z"
    }
   },
   "id": "ae7c30f869273472",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a very short JSON file:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"name\": \"John\",\n",
      "  \"age\": 30\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Generate very short JSON file.\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T13:11:59.250943300Z",
     "start_time": "2024-07-24T13:11:57.962397800Z"
    }
   },
   "id": "715dde0d5757e2c5",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Class for DeepEval"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19555a1527d13e2b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class CustomLlama3_8B(DeepEvalBaseLLM):\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.terminators = [\n",
    "            tokenizer.eos_token_id,\n",
    "            tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "        \n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=256,\n",
    "            eos_token_id=self.terminators,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "        response = outputs[0][input_ids.shape[-1]:]\n",
    "        \n",
    "        return tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        return self.generate(prompt)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Llama-3 8B\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T13:14:20.351733600Z",
     "start_time": "2024-07-24T13:14:20.340732800Z"
    }
   },
   "id": "752d110d7b93df38",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "custom_LLM = CustomLlama3_8B(model, tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T13:16:42.997398500Z",
     "start_time": "2024-07-24T13:16:42.994885400Z"
    }
   },
   "id": "5c54474a919a3273",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n"
     ]
    }
   ],
   "source": [
    "print(custom_LLM.generate(\"Tell a joke.\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T13:17:10.302275300Z",
     "start_time": "2024-07-24T13:17:09.367350300Z"
    }
   },
   "id": "fd27ce1cdf29a15e",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "### DeepEval metrics\n",
    "#### Hallucination"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c60e1d8b5fa31589"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Output()",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5bb0aeb9fc114e609e05c0428f57d3fe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": "",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Output()",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec78f7bed1dc4cdea4311a5092379938"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "The score is 0.00 because the actual output perfectly aligns with the provided context, indicating no hallucination.\n",
      "Evaluating test cases...\n",
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": "",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Hallucination (score: 0.0, threshold: 0.5, strict: False, evaluation model: Llama-3 8B, reason: The score is 0.00 because the actual output does not contain any information that contradicts the provided context, which mentions a blond-hair person., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What was the blond doing?\n",
      "  - actual output: A blond drinking water in public.\n",
      "  - expected output: None\n",
      "  - context: ['A man with blond-hair, and a brown shirt drinking out of a public water fountain.']\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Hallucination: 100.00% pass rate\n",
      "\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "✅ Tests finished! Run \u001B[32m\"deepeval login\"\u001B[0m to view evaluation results on the web.\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Tests finished! Run <span style=\"color: #008000; text-decoration-color: #008000\">\"deepeval login\"</span> to view evaluation results on the web.\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[TestResult(success=True, metrics_metadata=[MetricMetadata(metric='Hallucination', threshold=0.5, success=True, score=0.0, reason='The score is 0.00 because the actual output does not contain any information that contradicts the provided context, which mentions a blond-hair person.', strict_mode=False, evaluation_model='Llama-3 8B', error=None, evaluation_cost=None, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The actual output agrees with the provided context which mentions a blond-hair person.\"\\n    }\\n]')], input='What was the blond doing?', actual_output='A blond drinking water in public.', expected_output=None, context=['A man with blond-hair, and a brown shirt drinking out of a public water fountain.'], retrieval_context=None)]"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import HallucinationMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Replace this with the actual documents that you are passing as input to your LLM.\n",
    "context=[\"A man with blond-hair, and a brown shirt drinking out of a public water fountain.\"]\n",
    "\n",
    "# Replace this with the actual output from your LLM application\n",
    "actual_output=\"A blond drinking water in public.\"\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What was the blond doing?\",\n",
    "    actual_output=actual_output,\n",
    "    context=context\n",
    ")\n",
    "metric = HallucinationMetric(threshold=0.5, model=custom_LLM)\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "# or evaluate test cases in bulk\n",
    "evaluate([test_case], [metric])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T13:22:39.594299500Z",
     "start_time": "2024-07-24T13:22:33.022016700Z"
    }
   },
   "id": "99f3524d8e3c2102",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "12733945486480b6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
