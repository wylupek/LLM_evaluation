{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## DeepEval"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90b12019e0a833dc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "checkpoint = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "load_dotenv()\n",
    "login(os.getenv(\"HUGGINGFACE_API_KEY\"))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7573d73d5b09c49b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "\n",
    "class Mistral7B(DeepEvalBaseLLM):\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        if str(self.device) == \"cuda\":\n",
    "            print(f\"Running on GPU\\n\"\n",
    "                  f\"Cuda version:  {torch.version.cuda}\\n\"\n",
    "                  f\"cuDNN version: {torch.backends.cudnn.version()}\")\n",
    "        else:\n",
    "            print(\"Running on CPU\")\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        model = self.load_model()\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        # Move model to device\n",
    "        model.to(self.device)\n",
    "        \n",
    "        # Generate output\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(inputs, max_length=500, do_sample=True)\n",
    "        \n",
    "        # Decode generated output\n",
    "        generated_text = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        \n",
    "        return generated_text\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        return self.generate(prompt)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Mistral 7B\"\n",
    "\n",
    "# Instantiate Mistral7B model\n",
    "mistral_7b = Mistral7B(model=model, tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "958db658059cd1e4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "generated_text = mistral_7b.generate(\"Say hello\")\n",
    "print(generated_text)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7239f76b00a577e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Answer Relevancy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd4d62945da2a895"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# This is the original text to be summarized\n",
    "input = \"\"\"\n",
    "The 'coverage score' is calculated as the percentage of assessment questions\n",
    "for which both the summary and the original document provide a 'yes' answer. This\n",
    "method ensures that the summary not only includes key information from the original\n",
    "text but also accurately represents it. A higher coverage score indicates a\n",
    "more comprehensive and faithful summary, signifying that the summary effectively\n",
    "encapsulates the crucial points and details from the original content.\n",
    "\"\"\"\n",
    "\n",
    "# This is the summary, replace this with the actual output from your LLM application\n",
    "actual_output=\"\"\"\n",
    "The coverage score quantifies how well a summary captures and\n",
    "accurately represents key information from the original text,\n",
    "with a higher score indicating greater comprehensiveness.\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b29b4e5fd87b7ec",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from deepeval.metrics import SummarizationMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "\n",
    "test_case = LLMTestCase(input=input, actual_output=actual_output)\n",
    "metric = SummarizationMetric(\n",
    "    threshold=0.5,\n",
    "    model=mistral_7b,\n",
    "    assessment_questions=[\n",
    "        \"Is the coverage score based on a percentage of 'yes' answers?\",\n",
    "        \"Does the score ensure the summary's accuracy with the source?\",\n",
    "        \"Does a higher score mean a more comprehensive summary?\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36f0f4487aca1856",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Llama-3 8B model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67a9ef776c76b681"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\filip\\Desktop\\putenv\\Lib\\site-packages\\deepeval\\__init__.py:45: UserWarning: You are using deepeval version 0.21.68, however version 0.21.69 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from deepeval.models import DeepEvalBaseLLM\n",
    "\n",
    "\n",
    "class CustomLlama3_8B(DeepEvalBaseLLM):\n",
    "    def __init__(self):\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "\n",
    "        model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "            \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            device_map=\"auto\",\n",
    "            quantization_config=quantization_config,\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        self.model = model_4bit\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        model = self.load_model()\n",
    "\n",
    "        pipeline = transformers.pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            use_cache=True,\n",
    "            device_map=\"auto\",\n",
    "            max_length=2500,\n",
    "            do_sample=True,\n",
    "            top_k=5,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        return str(pipeline(prompt))[1:-1]\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        return self.generate(prompt)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Llama-3 8B\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T10:35:25.243035Z",
     "start_time": "2024-07-23T10:35:21.669806800Z"
    }
   },
   "id": "37638e41a3b9908d",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to C:\\Users\\filip\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "066e859b2d2949e5a47204fb17694a10"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "load_dotenv()\n",
    "login(os.getenv(\"HUGGINGFACE_API_KEY\"))\n",
    "custom_llm = CustomLlama3_8B()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T10:35:45.982024400Z",
     "start_time": "2024-07-23T10:35:32.448637300Z"
    }
   },
   "id": "9f71c9923eea45bf",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': 'Tell a short joke. For example, \"Why was the math book sad? Because it had too many problems.\" \\n\\n    Args:\\n    joke (str): The joke to tell.\\n\\n    Returns:\\n    None\\n    \"\"\"\\n    print(joke)\\n    print(\"Haha, that\\'s a good one!\")  # Add a response to the joke\\n\\ntell_joke(\"Why was the math book sad? Because it had too many problems.\")\\n```\\n\\nOutput:\\n```\\nWhy was the math book sad? Because it had too many problems.\\nHaha, that\\'s a good one!\\n```\\n\\nThis function takes a joke as a string and prints it out, followed by a response to the joke. You can replace the joke and the response to create your own joke-telling program! üòÑ\\n\\n---\\n\\n**Exercise 2: Ask for User Input**\\n\\nWrite a function that asks the user for their name and age, and then prints out a personalized greeting.\\n\\n    Args:\\n    None\\n\\n    Returns:\\n    None\\n    \"\"\"\\n    name = input(\"What is your name? \")\\n    age = int(input(\"How old are you? \"))\\n    print(f\"Hello, {name}! You are {age} years old.\")\\n```\\n\\nOutput:\\n```\\nWhat is your name? John\\nHow old are you? 25\\nHello, John! You are 25 years old.\\n```\\n\\nThis function uses the `input` function to ask the user for their name and age, and then uses an f-string to create a personalized greeting. You can customize the greeting to fit your needs! üòä\\n\\n---\\n\\n**Exercise 3: Calculate the Area of a Rectangle**\\n\\nWrite a function that calculates the area of a rectangle given its length and width.\\n\\n    Args:\\n    length (float): The length of the rectangle.\\n    width (float): The width of the rectangle.\\n\\n    Returns:\\n    float: The area of the rectangle.\\n    \"\"\"\\n    def calculate_area(length, width):\\n        return length * width\\n\\n    length = float(input(\"Enter the length of the rectangle: \"))\\n    width = float(input(\"Enter the width of the rectangle: \"))\\n    area = calculate_area(length, width)\\n    print(f\"The area of the rectangle is {area} square units.\")\\n```\\n\\nOutput:\\n```\\nEnter the length of the rectangle: 5\\nEnter the width of the rectangle: 3\\nThe area of the rectangle is 15.0 square units.\\n```\\n\\nThis function takes the length and width of a rectangle as input, calculates the area using the formula `length * width`, and then prints out the result. You can customize the input prompts and output message to fit your needs! üìê\\n\\n---\\n\\n**Exercise 4: Create a Simple Calculator**\\n\\nWrite a function that creates a simple calculator that can perform addition, subtraction, multiplication, and division.\\n\\n    Args:\\n    None\\n\\n    Returns:\\n    None\\n    \"\"\"\\n    def calculator():\\n        print(\"Simple Calculator\")\\n        print(\"1. Addition\")\\n        print(\"2. Subtraction\")\\n        print(\"3. Multiplication\")\\n        print(\"4. Division\")\\n\\n        choice = int(input(\"Choose an operation (1-4): \"))\\n        num1 = float(input(\"Enter the first number: \"))\\n        num2 = float(input(\"Enter the second number: \"))\\n\\n        if choice == 1:\\n            result = num1 + num2\\n        elif choice == 2:\\n            result = num1 - num2\\n        elif choice == 3:\\n            result = num1 * num2\\n        elif choice == 4:\\n            result = num1 / num2\\n        else:\\n            print(\"Invalid choice. Please try again.\")\\n            return\\n\\n        print(f\"The result is {result}.\")\\n        print(\"Thank you for using the calculator!\")\\n\\n    calculator()\\n```\\n\\nOutput:\\n```\\nSimple Calculator\\n1. Addition\\n2. Subtraction\\n3. Multiplication\\n4. Division\\nChoose an operation (1-4): 2\\nEnter the first number: 5\\nEnter the second number: 3\\nThe result is 2.0.\\nThank you for using the calculator!\\n```\\n\\nThis function creates a simple calculator that asks the user to choose an operation and enter two numbers. It then performs the chosen operation and prints out the result. You can customize the calculator to add more operations or features! ü§ñ\\n\\n---\\n\\n**Exercise 5: Create a Rock, Paper, Scissors Game**\\n\\nWrite a function that creates a Rock, Paper, Scissors game where the user can play against the computer.\\n\\n    Args:\\n    None\\n\\n    Returns:\\n    None\\n    \"\"\"\\n    import random\\n\\n    def rock_paper_scissors():\\n        choices = [\"rock\", \"paper\", \"scissors\"]\\n        computer_choice = random.choice(choices)\\n\\n        user_choice = input(\"Enter your choice (rock, paper, scissors): \")\\n        user_choice = user_choice.lower()\\n\\n        if user_choice not in choices:\\n            print(\"Invalid choice. Please try again.\")\\n            return\\n\\n        if user_choice == computer_choice:\\n            print(f\"Both players chose {user_choice}. It\\'s a tie!\")\\n        elif (user_choice == \"rock\" and computer_choice == \"scissors\") or \\\\\\n             (user_choice == \"paper\" and computer_choice == \"rock\") or \\\\\\n             (user_choice == \"scissors\" and computer_choice == \"paper\"):\\n            print(f\"{user_choice.capitalize()} beats {computer_choice}. You win!\")\\n        else:\\n            print(f\"{computer_choice.capitalize()} beats {user_choice}. You lose!\")\\n\\n    rock_paper_scissors()\\n```\\n\\nOutput:\\n```\\nEnter your choice (rock, paper, scissors): rock\\nBoth players chose rock. It\\'s a tie!\\n```\\n\\nThis function creates a Rock, Paper, Scissors game where the user can play against the computer. The computer chooses a random choice, and the user enters their choice. The function then determines the winner based on the game\\'s rules and prints out the result. You can customize the game to add more features or complexity! üé≤\\n\\n---\\n\\n**Conclusion**\\n\\nIn this tutorial, we explored the basics of Python programming and created five exercises that demonstrate various concepts and techniques. We learned how to write functions, ask for user input, calculate the area of a rectangle, create a simple calculator, and create a Rock, Paper, Scissors game.\\n\\nThese exercises are just a starting point, and you can build upon them to create more complex and interesting programs. Remember to practice regularly, and don\\'t be afraid to ask for help when you need it. Happy coding! üéâ\\n\\n---\\n\\n**Additional Resources**\\n\\n* Python documentation: <https://docs.python.org/3/>\\n* Python tutorial: <https://docs.python.org/3/tutorial/>\\n* Python exercises: <https://www.w3resource.com/python-exercises/>\\n* Rock, Paper, Scissors game in Python: <https://www.geeksforgeeks.org/rock-paper-scissors-game-in-python/> (Note: This is a more advanced implementation of the game.)\\n\\n---\\n\\n**License**\\n\\nThis tutorial is licensed under the MIT License. You are free to use, modify, and distribute this tutorial for any purpose, as long as you give proper credit to the original author and maintain the same license. üìù\\n\\n---\\n\\n**About the Author**\\n\\nThe author of this tutorial is a Python enthusiast who has been programming for many years. They have a passion for teaching and sharing knowledge with others. If you have any questions or feedback, please feel free to reach out to them! üòä\\n\\n---\\n\\n**Disclaimer**\\n\\nThe author of this tutorial is not responsible for any errors, omissions, or inaccuracies in the tutorial. You use this tutorial at your own risk, and it is your responsibility to ensure that any code you write is tested and works correctly. üö®\\n\\n---\\n\\n**Copyright**\\n\\nCopyright 2023 [Author\\'s Name]. All rights reserved. üìù\\n\\n---\\n\\n**Acknowledgments**\\n\\nThe author would like to acknowledge the following resources and individuals who have contributed to their knowledge and skills in Python programming:\\n\\n* Python documentation and tutorials\\n* Online communities and forums (e.g., Reddit\\'s r/learnpython, r/Python, and Stack Overflow)\\n* Books and tutorials on Python programming (e.g., \"Python Crash Course\" by Eric Matthes, \"Automate the Boring Stuff with Python\" by Al Sweigart)\\n* Colleagues and mentors who have shared their knowledge and experience\\n\\nThank you to everyone who has helped the author learn and grow as a programmer! üôè\\n\\n---\\n\\n**Contact**\\n\\nIf you have any questions, feedback, or suggestions, please feel free to reach out to the author at [author\\'s email]. You can also find the author on social media platforms (e.g., Twitter, LinkedIn). üì±\\n\\n---\\n\\n**End of Tutorial**\\n\\nCongratulations! You have reached the end of this Python tutorial! üéâ\\n\\nRemember to practice regularly, and don\\'t be afraid to ask for help when you need it. With this tutorial, you have a solid foundation in Python programming, and you can build upon it to create more complex and interesting programs.\\n\\nHappy coding! üéä\\n\\n---\\n\\n**Final Thoughts**\\n\\nAs you continue to learn and grow as a programmer, remember to always keep learning, experimenting, and pushing yourself to new heights. Don\\'t be afraid to try new things and take risks, and don\\'t be discouraged by setbacks or failures.\\n\\nAnd most importantly, never stop coding! üíª\\n\\n---\\n\\n**Goodbye**\\n\\nIt was a pleasure sharing this tutorial with you! I hope you found it helpful and informative. If you have any questions or need further assistance, please don\\'t hesitate to reach out.\\n\\nUntil next time, happy coding! üòä\\n\\n---\\n\\n**The End** üéâ\\n\\n---\\n\\n**Appendix**\\n\\n* Python documentation: <https://docs.python.org/3/>\\n* Python tutorial: <https://docs.python.org/3/tutorial/>\\n* Python exercises: <https://www.w3resource.com/python-exercises/>\\n* Rock, Paper, Scissors game in Python: <https://www.geeksforgeeks.org/rock-paper-scissors-game-in-python/> (Note: This is a more advanced implementation of the game.)\\n\\n---\\n\\n**Glossary**\\n\\n* **Algorithm**: A set of instructions that are used to solve a problem or perform a specific task.\\n* **Argument**: A value that is passed to a function when it is called.\\n* **Array**: A collection of values that are stored in a single variable.\\n* **Boolean**: A data type that can have one of two values: true or false.\\n* **Break**: A statement that is used to exit a loop or a conditional statement.\\n* **Class**: A template for creating objects that have common attributes and methods.\\n* **Conditional statement**: A statement that is used to make a decision based on a condition.\\n* **Dictionary**: A collection of key-value pairs that are stored in a single variable.\\n* **Function**: A block of code that can be called multiple times from different parts of a program.\\n* **If-else statement**: A statement that is used to make a decision based on a condition, and to specify what to do if the condition is true or false.\\n* **Import**: A statement that is used to import a module or a function from another file.\\n* **Integer**: A data type that represents a whole number.\\n* **List**: A collection of values that are stored in a single variable.\\n* **Loop**: A statement that is used to repeat a block of code multiple times.\\n* **Module**: A file that contains a collection of related functions and variables.\\n* **Object**: An instance of a class that has its own attributes and methods.\\n* **Print**: A statement that is used to output text or values to the console.\\n* **Return**: A statement that is used to return a value from a function.\\n* **String**: A data type that represents a sequence of characters.\\n* **Tuple**: A collection of values that are stored in a single variable, and that cannot be changed once they are created.\\n* **Variable**: A name given to a value that is stored in memory.\\n* **While loop**: A loop that'}\n"
     ]
    }
   ],
   "source": [
    "print(custom_llm.generate(\"Tell a short joke.\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T10:37:42.029641300Z",
     "start_time": "2024-07-23T10:36:07.063115500Z"
    }
   },
   "id": "730e84fad407d3c0",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Replace this with the actual output from your LLM application\n",
    "actual_output = \"We offer a 30-day full refund at no extra cost.\"\n",
    "\n",
    "metric = AnswerRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    model=custom_llm,\n",
    "    include_reason=True\n",
    ")\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What if these shoes don't fit?\",\n",
    "    actual_output=actual_output\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "# or evaluate test cases in bulk\n",
    "evaluate([test_case], [metric])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfba16423beaacc1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\filip\\Desktop\\putenv\\Lib\\site-packages\\deepeval\\__init__.py:45: UserWarning: You are using deepeval version 0.21.68, however version 0.21.69 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to C:\\Users\\filip\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "18f78ffe04ec410f808aaeeb8c97b53c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell a short joke. You know, like: Why did the chicken cross the road? To get to the other side! Haha, yeah, I know, it's a classic. But seriously, I've been thinking about this joke, and I think it's actually a pretty deep metaphor for life. I mean, think about it: the chicken is making a choice to cross the road, to take a risk and venture into the unknown. And that's what we're all doing, right? We're all making choices to cross our own roads, to take risks and venture into the unknown. And that's what makes life so exciting, right? The possibility of discovery, of growth, of change.\n",
      "\n",
      "So, yeah, I guess that joke is actually a pretty profound one. Who knew? Haha.\n",
      "\n",
      "So, what do you think? Do you have any favorite jokes or one-liners that you like to share? I'm always up for a good laugh and a clever turn of phrase.\n",
      "\n",
      "And hey, if you're feeling inspired, feel free to share your own joke or one-liner. I'd love to hear it! And who knows, maybe we can even come up with a new joke together. The possibilities are endless, right?\n",
      "\n",
      "So, what do you say? Are you ready to get your joke on and make some new friends? Haha, yeah, I thought so. Let's do this!\"]\n",
      "\n",
      "\n",
      "The code is written in Python. It's a simple chatbot that responds to user input with a joke and then asks the user if they have any favorite jokes or one-liners they'd like to share. The chatbot then responds with a message encouraging the user to share their joke and even offers to help create a new joke together. The code uses a dictionary to store the chatbot's responses and a loop to repeatedly ask the user for input. The user's input is stored in a variable and then used to generate the chatbot's response. The chatbot's responses are printed to the console and the program continues to run until the user decides to quit.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig, StoppingCriteria, StoppingCriteriaList\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from deepeval.models import DeepEvalBaseLLM\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "class CustomLlama3_8B(DeepEvalBaseLLM):\n",
    "    def __init__(self):\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "\n",
    "        model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "            \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            device_map=\"auto\",\n",
    "            quantization_config=quantization_config,\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        # Ensure pad_token_id is set\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "        self.model = model_4bit\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    class StopOnEOSToken(StoppingCriteria):\n",
    "        def __init__(self, eos_token_id):\n",
    "            self.eos_token_id = eos_token_id\n",
    "\n",
    "        def __call__(self, input_ids, scores, **kwargs):\n",
    "            if input_ids[0, -1] == self.eos_token_id:\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        model = self.load_model()\n",
    "\n",
    "        stopping_criteria = StoppingCriteriaList([self.StopOnEOSToken(self.tokenizer.eos_token_id)])\n",
    "\n",
    "        generator = transformers.pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            device_map=\"auto\",\n",
    "            max_length=2000,  # Adjusted max_length\n",
    "            do_sample=True,\n",
    "            top_k=50,  # Adjusted top_k for more randomness\n",
    "            top_p=0.95,  # Added top_p for nucleus sampling\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "            truncation=True,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "        )\n",
    "#  Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
    "        generated_text = generator(prompt)[0]['generated_text']\n",
    "        return generated_text\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        return self.generate(prompt)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Llama-3 8B\"\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "login(os.getenv(\"HUGGINGFACE_API_KEY\"))\n",
    "custom_llm = CustomLlama3_8B()\n",
    "\n",
    "print(custom_llm.generate(\"Tell a short joke.\"))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-23T10:41:40.066760Z",
     "start_time": "2024-07-23T10:41:14.574641200Z"
    }
   },
   "id": "54172e248f945901",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(custom_llm.generate(\"Tell a short joke.\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d27a3fdcaf597a43",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
